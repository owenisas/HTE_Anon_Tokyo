[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "watermark-llamacpp"
version = "0.1.0"
description = "Hybrid watermark gateway for llama.cpp"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "fastapi>=0.115.0",
  "httpx>=0.27.0",
  "pydantic>=2.8.0",
]

[project.optional-dependencies]
vllm = ["vllm>=0.6.0"]
sglang = ["sglang>=0.4.0"]
mlx-lm = ["mlx-lm>=0.20.0"]
llama-cpp = ["llama-cpp-python>=0.3.0"]
transformers = ["transformers>=4.45.0"]
engines-all = [
  "vllm>=0.6.0",
  "sglang>=0.4.0",
  "mlx-lm>=0.20.0",
  "llama-cpp-python>=0.3.0",
  "transformers>=4.45.0",
]

[project.scripts]
watermark-llamacpp-gateway = "watermark_llamacpp.gateway:main"
watermark-install-engines = "watermark_llamacpp.install_engines:main"

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]
